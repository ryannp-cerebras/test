{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryannp-cerebras/test/blob/main/Build_your_own_Perplexity_%7C_Cerebras_and_Exa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build your own Perplexity | Cerebras and Exa\n",
        "\n",
        "Today, we'll build a Perplexity-style deep research assistant that can automatically search the web, analyzes multiple sources, and provide structured insights in under 60 seconds.\n",
        "\n",
        "Try out the full deep research chat [here](https://cerebras-deepresearch.netlify.app).\n",
        "\n",
        "\n",
        "\n",
        "For this workshop, you'll need:\n",
        "- Cerebras API: the fastest inference provider, [get started for free here](https://cloud.cerebras.ai?utm_source=exademo)\n",
        "- Exa API: The search engine for AI, [get started for free here](https://exa.ai/?utm_source=cerebras-research)\n",
        "\n",
        "If you have any questions, please reach out on the [Cerebras Discord](https://cerebras.ai/discord)"
      ],
      "metadata": {
        "id": "IWoG5UXdpoDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #1: Environment Setup\n",
        "First, let's install all the necessary libraries, import everything we need, and configure our API credentials."
      ],
      "metadata": {
        "id": "umRieFrUBCFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ7hFZwmpkmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6c9917-7595-400d-ef45-a980e5021214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting exa-py\n",
            "  Downloading exa_py-1.14.12-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting cerebras-cloud-sdk\n",
            "  Downloading cerebras_cloud_sdk-1.35.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from exa-py) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from exa-py) (4.14.0)\n",
            "Requirement already satisfied: openai>=1.48 in /usr/local/lib/python3.11/dist-packages (from exa-py) (1.87.0)\n",
            "Requirement already satisfied: pydantic>=2.10.6 in /usr/local/lib/python3.11/dist-packages (from exa-py) (2.11.7)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from exa-py) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from cerebras-cloud-sdk) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->cerebras-cloud-sdk) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->exa-py) (0.16.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (0.10.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->exa-py) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->exa-py) (2.4.0)\n",
            "Downloading exa_py-1.14.12-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cerebras_cloud_sdk-1.35.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cerebras-cloud-sdk, exa-py\n",
            "Successfully installed cerebras-cloud-sdk-1.35.0 exa-py-1.14.12\n",
            "✅ Setup complete\n"
          ]
        }
      ],
      "source": [
        "%pip install exa-py cerebras-cloud-sdk\n",
        "\n",
        "from exa_py import Exa\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "\n",
        "# Add your API keys here\n",
        "EXA_API_KEY = \"\"\n",
        "CEREBRAS_API_KEY = \"\"\n",
        "\n",
        "client = Cerebras(api_key = CEREBRAS_API_KEY)\n",
        "exa = Exa(api_key = EXA_API_KEY)\n",
        "\n",
        "print(\"✅ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1oG88UGDzu14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #2: Web Search Function\n",
        "\n",
        "Our first core function handles web searching using Exa's auto search. This is advantageous because it uses a blend of keyword and neural search to find both exact matches and semantic similarities. It also returns the content of each scraped URL."
      ],
      "metadata": {
        "id": "kFmPpHAUBZxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_web(query, num=5):\n",
        "    \"\"\"Search the web using Exa's auto search\"\"\"\n",
        "    result = exa.search_and_contents(\n",
        "      query,\n",
        "      type = \"auto\",\n",
        "      num_results = num,\n",
        "      text={\"max_characters\": 1000}\n",
        "    )\n",
        "    return result.results\n",
        "\n",
        "print(\"✅ Search function ready\")"
      ],
      "metadata": {
        "id": "ELTKr4E-zxNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660d1a6c-c565-4655-eb9b-684d8714a549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Search function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #3: AI Analysis Function\n",
        "\n",
        "This function leverages Cerebras fast inference to analyze content and generate insights. We'll use it both for structured JSON responses and regular text analysis throughout our research process."
      ],
      "metadata": {
        "id": "IL3bsNPQz-ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_ai(prompt):\n",
        "    \"\"\"Get AI response from Cerebras\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"llama-4-scout-17b-16e-instruct\",\n",
        "      max_tokens = 600,\n",
        "      temperature = 0.2\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "print(\"✅ AI function ready\")"
      ],
      "metadata": {
        "id": "yyIMf2p40CEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93486079-1c1a-4f0e-a333-a56520aa8e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AI function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step #4: Research Function\n",
        "\n",
        "Now we'll build our research methodology. The first part of the below cell performs the initial search and gathers our first batch of sources, just like when you first query Perplexity.\n",
        "\n",
        "Then, the AI is queried to generate a conclusion based on the source data."
      ],
      "metadata": {
        "id": "CFBhRi-q0Nf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def research_topic(query):\n",
        "    \"\"\"Main research function that orchestrates the entire process\"\"\"\n",
        "    print(f\"🔍 Researching: {query}\")\n",
        "\n",
        "    # Search for sources\n",
        "    results = search_web(query, 5)\n",
        "    print(f\"📊 Found {len(results)} sources\")\n",
        "\n",
        "    # Get content from sources\n",
        "    sources = []\n",
        "    for result in results:\n",
        "        content = result.text\n",
        "        title = result.title\n",
        "        if content and len(content) > 200:\n",
        "            sources.append({\n",
        "                \"title\": title,\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "    print(f\"📄 Scraped {len(sources)} sources\")\n",
        "\n",
        "    if not sources:\n",
        "        return {\"summary\": \"No sources found\", \"insights\": []}\n",
        "\n",
        "    # Create context for AI analysis\n",
        "    context = f\"Research query: {query}\\n\\nSources:\\n\"\n",
        "    for i, source in enumerate(sources[:4], 1):\n",
        "        context += f\"{i}. {source['title']}: {source['content'][:400]}...\\n\\n\"\n",
        "        # ^^ get rid of this to use API params!\n",
        "        # best practices - https://www.anthropic.com/engineering/built-multi-agent-research-system\n",
        "\n",
        "    # Ask AI to analyze and synthesize\n",
        "    prompt = f\"\"\"{context}\n",
        "\n",
        "Based on these sources, provide:\n",
        "1. A comprehensive summary (2-3 sentences)\n",
        "2. Three key insights as bullet points\n",
        "\n",
        "Format your response exactly like this:\n",
        "SUMMARY: [your summary here]\n",
        "\n",
        "INSIGHTS:\n",
        "- [insight 1]\n",
        "- [insight 2]\n",
        "- [insight 3]\"\"\"\n",
        "\n",
        "    response = ask_ai(prompt)\n",
        "    print(\"🧠 Analysis complete\")\n",
        "\n",
        "    return {\"query\": query, \"sources\": len(sources), \"response\": response}\n",
        "\n",
        "print(\"✅ Research function ready\")"
      ],
      "metadata": {
        "id": "tyvMwjl40WkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fa496f-c5aa-41d7-938a-fbf9dd6c1a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Research function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step #5: Add Research Depth\n",
        "\n",
        "Now let's make our research **intelligent** instead of just searching once and hoping for the best.\n",
        "\n",
        "Here's the problem with basic search: You ask \"What are the latest AI breakthroughs?\" and get random articles. But what if those articles all focus on ChatGPT and miss robotics? You'd never know what you're missing.\n",
        "\n",
        "Here is a more advanced research flow.\n",
        "\n",
        "```\n",
        "You ask: \"What's driving the new wave of AI agents?\"\n",
        " ↓\n",
        "Layer 1: Broad search finds 6 sources about agent frameworks like AutoGPT, LangChain, and OpenAI’s GPT-4o\n",
        " ↓\n",
        "AI reads them and thinks: “These all focus on software… but what’s enabling real-time speed?”\n",
        " ↓\n",
        "Layer 2: Targeted search for \"AI hardware for real-time agents\" and \"fast inference for LLMs\"\n",
        " ↓\n",
        "Final synthesis: Combines insights on agent software + breakthroughs in inference speed (Cerebras, NVIDIA, etc) =\n",
        "A complete picture of what powers real-time AI agents today\n",
        "```"
      ],
      "metadata": {
        "id": "8Flvk76_0X_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deeper_research_topic(query):\n",
        "    \"\"\"Two-layer research for better depth\"\"\"\n",
        "    print(f\"🔍 Researching: {query}\")\n",
        "\n",
        "    # Layer 1: Initial search\n",
        "    results = search_web(query, 6)\n",
        "    sources = []\n",
        "    for result in results:\n",
        "        if result.text and len(result.text) > 200:\n",
        "            sources.append({\"title\": result.title, \"content\": result.text})\n",
        "\n",
        "    print(f\"Layer 1: Found {len(sources)} sources\")\n",
        "\n",
        "    if not sources:\n",
        "        return {\"summary\": \"No sources found\", \"insights\": []}\n",
        "\n",
        "    # Get initial analysis and identify follow-up topic\n",
        "    context1 = f\"Research query: {query}\\n\\nSources:\\n\"\n",
        "    for i, source in enumerate(sources[:4], 1):\n",
        "        context1 += f\"{i}. {source['title']}: {source['content'][:300]}...\\n\\n\"\n",
        "\n",
        "    follow_up_prompt = f\"\"\"{context1}\n",
        "\n",
        "Based on these sources, what's the most important follow-up question that would deepen our understanding of \"{query}\"?\n",
        "\n",
        "Respond with just a specific search query (no explanation):\"\"\"\n",
        "\n",
        "    follow_up_query = ask_ai(follow_up_prompt).strip().strip('\"')\n",
        "\n",
        "    # Layer 2: Follow-up search\n",
        "    print(f\"Layer 2: Investigating '{follow_up_query}'\")\n",
        "    follow_results = search_web(follow_up_query, 4)\n",
        "\n",
        "    for result in follow_results:\n",
        "        if result.text and len(result.text) > 200:\n",
        "            sources.append({\"title\": f\"[Follow-up] {result.title}\", \"content\": result.text})\n",
        "\n",
        "    print(f\"Total sources: {len(sources)}\")\n",
        "\n",
        "    # Final synthesis\n",
        "    all_context = f\"Research query: {query}\\nFollow-up: {follow_up_query}\\n\\nAll Sources:\\n\"\n",
        "    for i, source in enumerate(sources[:7], 1):\n",
        "        all_context += f\"{i}. {source['title']}: {source['content'][:300]}...\\n\\n\"\n",
        "\n",
        "    final_prompt = f\"\"\"{all_context}\n",
        "\n",
        "Provide a comprehensive analysis:\n",
        "\n",
        "SUMMARY: [3-4 sentences covering key findings from both research layers]\n",
        "\n",
        "INSIGHTS:\n",
        "- [insight 1]\n",
        "- [insight 2]\n",
        "- [insight 3]\n",
        "- [insight 4]\n",
        "\n",
        "DEPTH GAINED: [1 sentence on how the follow-up search enhanced understanding]\"\"\"\n",
        "\n",
        "    response = ask_ai(final_prompt)\n",
        "    return {\"query\": query, \"sources\": len(sources), \"response\": response}\n",
        "\n",
        "print(\"✅ Enhanced research function ready\")"
      ],
      "metadata": {
        "id": "Pnyqp3440d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee797230-c865-48ba-aa2a-a27c35649694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced research function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the enhanced research system\n",
        "result = deeper_research_topic(\"climate change solutions 2025\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ENHANCED RESEARCH RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Query: {result['query']}\")\n",
        "print(f\"Sources analyzed: {result['sources']}\")\n",
        "print(f\"\\n{result['response']}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Try more topics\n",
        "print(\"\\nTry these:\")\n",
        "print(\"deeper_research_topic('quantum computing advances')\")\n",
        "print(\"deeper_research_topic('space exploration news')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Qo5GCufKmB",
        "outputId": "c6259564-a13d-4a70-a0fd-6c4b86fcb42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Researching: climate change solutions 2025\n",
            "Layer 1: Found 6 sources\n",
            "Layer 2: Investigating 'What are the top 3 climate change solutions proposed for 2025 by MIT Solve, UNDP, and UNFCCC?'\n",
            "Total sources: 10\n",
            "\n",
            "==================================================\n",
            "ENHANCED RESEARCH RESULTS\n",
            "==================================================\n",
            "Query: climate change solutions 2025\n",
            "Sources analyzed: 10\n",
            "\n",
            "**SUMMARY**\n",
            "\n",
            "The research query on climate change solutions for 2025 revealed various initiatives and proposals from prominent organizations. MIT Solve, UNDP, and UNFCCC are actively seeking and promoting innovative solutions to address climate change. The top 3 climate change solutions proposed for 2025 by these organizations focus on creating a zero-carbon world, adapting to a warming climate, and reducing greenhouse gas emissions. These solutions include leveraging technology, such as AI and graphene, to capture carbon and promote sustainable development.\n",
            "\n",
            "**INSIGHTS**\n",
            "\n",
            "* **MIT Solve's 2025 Global Climate Challenge**: The challenge focuses on creating a zero-carbon world and adapting to a warming climate, with a solution deadline of April 17, 2025.\n",
            "* **UNDP's Climate Promise 2025**: This initiative aims to support countries in achieving their Nationally Determined Contributions (NDCs) and promoting sustainable development.\n",
            "* **UNFCCC's Climate Week 2025**: The first Climate Week of 2025 concluded in Panama City, with progress made and real-world solutions shared across vital areas of work, including reducing greenhouse gas emissions.\n",
            "* **Technological innovations**: New research suggests that AI could reduce global emissions by 3.2 to 5.4 billion tonnes of carbon-dioxide-equivalent annually by 2035, while scalable graphene membranes and new techniques for turning ordinary rocks into carbon-capturing machines are being explored.\n",
            "\n",
            "**DEPTH GAINED**\n",
            "\n",
            "The follow-up search enhanced understanding by providing specific details on the top 3 climate change solutions proposed by MIT Solve, UNDP, and UNFCCC, highlighting the focus areas and initiatives of these organizations in addressing climate change in 2025.\n",
            "==================================================\n",
            "\n",
            "Try these:\n",
            "deeper_research_topic('quantum computing advances')\n",
            "deeper_research_topic('space exploration news')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step #6 (Optional): Anthropic Multi-Agent Research\n",
        "\n",
        "What makes [Anthropic's approach](https://www.anthropic.com/engineering/built-multi-agent-research-system) special?\n",
        "\n",
        "It uses intelligent orchestration with specialized agents working in parallel.\n",
        "\n",
        "Usually, one AI does everything sequentially\n",
        "- Search → analyze → search → analyze (slow, limited)\n",
        "\n",
        "Anthropic's approach is to allow a lead agent delegate to specialized subagents\n",
        "- Lead agent breaks down \"AI safety research\" into:\n",
        "  - Subagent 1: Current AI safety techniques  \n",
        "  - Subagent 2: Recent regulatory developments\n",
        "  - Subagent 3: Industry implementation challenges\n",
        "- All agents work simultaneously = 3x faster, better coverage\n",
        "\n",
        "The result is parallel intelligence that scales to complex research tasks."
      ],
      "metadata": {
        "id": "FnL-jWYjfM42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def anthropic_multiagent_research(query):\n",
        "    \"\"\"\n",
        "    Simple implementation of Anthropic's multi-agent approach:\n",
        "    1. Lead agent plans and delegates\n",
        "    2. Specialized subagents work in parallel\n",
        "    3. Lead agent synthesizes results\n",
        "    \"\"\"\n",
        "    print(f\"🤖 Anthropic Multi-Agent Research: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Step 1: Lead Agent - Task Decomposition & Delegation\n",
        "    print(\"👨‍💼 LEAD AGENT: Planning and delegating...\")\n",
        "\n",
        "    delegation_prompt = f\"\"\"You are a Lead Research Agent. Break down this complex query into 3 specialized subtasks for parallel execution: \"{query}\"\n",
        "\n",
        "For each subtask, provide:\n",
        "- Clear objective\n",
        "- Specific search focus\n",
        "- Expected output\n",
        "\n",
        "SUBTASK 1: [Core/foundational aspects]\n",
        "SUBTASK 2: [Recent developments/trends]\n",
        "SUBTASK 3: [Applications/implications]\n",
        "\n",
        "Make each subtask distinct to avoid overlap.\"\"\"\n",
        "\n",
        "    plan = ask_ai(delegation_prompt)\n",
        "    print(\"  ✓ Subtasks defined and delegated\")\n",
        "\n",
        "    # Step 2: Simulate Parallel Subagents (simplified for demo)\n",
        "    print(\"\\n🔍 SUBAGENTS: Working in parallel...\")\n",
        "\n",
        "    # Extract subtasks and create targeted searches\n",
        "    subtask_searches = [\n",
        "        f\"{query} fundamentals principles\",  # Core aspects\n",
        "        f\"{query} latest developments\",  # Recent trends\n",
        "        f\"{query} applications real world\"    # Implementation\n",
        "    ]\n",
        "\n",
        "    subagent_results = []\n",
        "    for i, search_term in enumerate(subtask_searches, 1):\n",
        "        print(f\"  🤖 Subagent {i}: Researching {search_term}\")\n",
        "        results = search_web(search_term, 2)\n",
        "\n",
        "        sources = []\n",
        "        for result in results:\n",
        "            if result.text and len(result.text) > 200:\n",
        "                sources.append({\n",
        "                    \"title\": result.title,\n",
        "                    \"content\": result.text[:300]\n",
        "                })\n",
        "\n",
        "        subagent_results.append({\n",
        "            \"subtask\": i,\n",
        "            \"search_focus\": search_term,\n",
        "            \"sources\": sources\n",
        "        })\n",
        "\n",
        "    total_sources = sum(len(r[\"sources\"]) for r in subagent_results)\n",
        "    print(f\"  📊 Combined: {total_sources} sources from {len(subagent_results)} agents\")\n",
        "\n",
        "    # Step 3: Lead Agent - Synthesis\n",
        "    print(\"\\n👨‍💼 LEAD AGENT: Synthesizing parallel findings...\")\n",
        "\n",
        "    # Combine all subagent findings\n",
        "    synthesis_context = f\"ORIGINAL QUERY: {query}\\n\\nSUBAGENT FINDINGS:\\n\"\n",
        "    for result in subagent_results:\n",
        "        synthesis_context += f\"\\nSubagent {result['subtask']} ({result['search_focus']}):\\n\"\n",
        "        for source in result['sources'][:2]:  # Limit for brevity\n",
        "            synthesis_context += f\"- {source['title']}: {source['content']}...\\n\"\n",
        "\n",
        "    synthesis_prompt = f\"\"\"{synthesis_context}\n",
        "\n",
        "As the Lead Agent, synthesize these parallel findings into a comprehensive report:\n",
        "\n",
        "EXECUTIVE SUMMARY:\n",
        "[2-3 sentences covering the most important insights across all subagents]\n",
        "\n",
        "INTEGRATED FINDINGS:\n",
        "• [Key finding from foundational research]\n",
        "• [Key finding from recent developments]\n",
        "• [Key finding from applications research]\n",
        "• [Cross-cutting insight that emerged]\n",
        "\n",
        "RESEARCH QUALITY:\n",
        "- Sources analyzed: {total_sources} across {len(subagent_results)} specialized agents\n",
        "- Coverage: [How well the subtasks covered the topic]\"\"\"\n",
        "\n",
        "    final_synthesis = ask_ai(synthesis_prompt)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"🎯 MULTI-AGENT RESEARCH COMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(final_synthesis)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"subagents\": len(subagent_results),\n",
        "        \"total_sources\": total_sources,\n",
        "        \"synthesis\": final_synthesis\n",
        "    }\n",
        "\n",
        "print(\"✅ Anthropic multi-agent system ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOzET3u8fOmv",
        "outputId": "b7bc95ed-8651-459e-9d5d-ad19578b0878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Anthropic multi-agent system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Anthropic multi-agent research system\n",
        "result = anthropic_multiagent_research(\"current climate change solutions\")\n",
        "\n",
        "print(\"\\n\" + \"🤖\" * 30)\n",
        "print(\"ANTHROPIC MULTI-AGENT DEMO\")\n",
        "print(\"🤖\" * 30)\n",
        "print(f\"Query: {result['query']}\")\n",
        "print(f\"Subagents deployed: {result['subagents']}\")\n",
        "print(f\"Total sources: {result['total_sources']}\")\n",
        "print(\"\\n💡 Key Innovation: Parallel specialized agents + intelligent orchestration\")\n",
        "print(\"\\n🎯 Try other complex topics:\")\n",
        "print(\"anthropic_multiagent_research('quantum computing commercial applications')\")\n",
        "print(\"anthropic_multiagent_research('artificial intelligence safety frameworks')\")\n",
        "print(\"anthropic_multiagent_research('renewable energy policy implementation')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqgqNroEfQ0_",
        "outputId": "476da18e-1956-4124-e9ab-6bb88f22e022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Anthropic Multi-Agent Research: current climate change solutions\n",
            "--------------------------------------------------\n",
            "👨‍💼 LEAD AGENT: Planning and delegating...\n",
            "  ✓ Subtasks defined and delegated\n",
            "\n",
            "🔍 SUBAGENTS: Working in parallel...\n",
            "  🤖 Subagent 1: Researching current climate change solutions fundamentals principles\n",
            "  🤖 Subagent 2: Researching current climate change solutions latest developments\n",
            "  🤖 Subagent 3: Researching current climate change solutions applications real world\n",
            "  📊 Combined: 6 sources from 3 agents\n",
            "\n",
            "👨‍💼 LEAD AGENT: Synthesizing parallel findings...\n",
            "\n",
            "==================================================\n",
            "🎯 MULTI-AGENT RESEARCH COMPLETE\n",
            "==================================================\n",
            "**EXECUTIVE SUMMARY:**\n",
            "The current climate change solutions landscape is rapidly evolving, with a focus on scaling up efforts to drive out fossil fuels and achieve net-zero emissions. Recent developments in solar-powered carbon capture and conversion technologies, as well as breakthroughs in electrochemical devices, are promising steps towards a low-carbon future. Real-world applications, such as direct air capture and coastal carbon capture, are also being implemented to address climate change.\n",
            "\n",
            "**INTEGRATED FINDINGS:**\n",
            "\n",
            "• **Foundational Principles**: The Exponential Roadmap Initiative and Nature Communications emphasize the need for clear definitions and principles to qualify climate solutions and climate solutions companies. Natural climate solutions, such as those involving plants, have been understood for over a century, but their implementation and scaling up are crucial.\n",
            "\n",
            "• **Recent Developments**: Researchers have made significant progress in developing solar-powered devices that can capture carbon dioxide from the air and convert it into sustainable fuel. Additionally, a breakthrough at Rice University has improved the stability of electrochemical devices that convert carbon dioxide into useful fuels and chemicals.\n",
            "\n",
            "• **Real-World Applications**: Companies like Carbon Engineering and Vesta are working on direct air capture and coastal carbon capture technologies, respectively. These solutions aim to pull carbon dioxide directly out of the atmosphere and create a low-carbon future.\n",
            "\n",
            "• **Cross-Cutting Insight**: A common thread across these findings is the importance of scaling up climate solutions, improving their efficiency, and making them more sustainable. This requires continued innovation, investment, and collaboration across sectors and industries.\n",
            "\n",
            "**RESEARCH QUALITY:**\n",
            "\n",
            "* Sources analyzed: 6 across 3 specialized agents\n",
            "* Coverage: The subtasks provided a comprehensive coverage of the topic, spanning foundational principles, recent developments, and real-world applications. The sources were diverse and included academic papers, research institutions, and company websites, providing a well-rounded view of current climate change solutions.\n",
            "\n",
            "🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖\n",
            "ANTHROPIC MULTI-AGENT DEMO\n",
            "🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖\n",
            "Query: current climate change solutions\n",
            "Subagents deployed: 3\n",
            "Total sources: 6\n",
            "\n",
            "💡 Key Innovation: Parallel specialized agents + intelligent orchestration\n",
            "\n",
            "🎯 Try other complex topics:\n",
            "anthropic_multiagent_research('quantum computing commercial applications')\n",
            "anthropic_multiagent_research('artificial intelligence safety frameworks')\n",
            "anthropic_multiagent_research('renewable energy policy implementation')\n"
          ]
        }
      ]
    }
  ]
}